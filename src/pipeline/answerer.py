
from typing import List, Dict
from src.modules.reranker import Reranker
from src.pipeline.searcher import Searcher
from src.core.utils import FileManager
import ollama   

class Answerer:
    """
    Answerer — High-level pipeline for generating answers in a multi-user RAG system.

    This class orchestrates the final step of a Retrieval-Augmented Generation (RAG) pipeline:
    1. Retrieve relevant document chunks for a given user query.
    2. Re-rank them using a cross-encoder.
    3. Concatenate them into a contextual passage.
    4. Generate a natural-language answer using a local Ollama LLM model.

    User isolation is guaranteed — each user only accesses their own stored chunks.
    """


    def __init__(self, config:dict, file_manager:FileManager, logger, user_id:str)-> None:
        """
        Initialize the Answerer for a specific user session.

        Args:
            config (dict): Configuration dictionary (e.g., loaded from config.yaml).
                Must contain:
                    - models.llm_model: name of the Ollama model to use (e.g., "llama3").
            file_manager (FileManager): File management utility for accessing local paths and configs.
            logger (Logger): Logger instance (e.g., Loguru) for logging system events.
            user_id (str): Unique identifier of the user to ensure private data isolation.
        """
        self.logger = logger
        self.files = file_manager
        self.user_id = user_id
        self.searcher = Searcher(config, file_manager, logger, user_id)
        self.model = config['models']["llm_model"]
        self.logger.info(f"Answer initialized for user {user_id} with Ollama model: {self.model}")



    def build_context(self, chunks:List[Dict])->str:
        """
        Combine multiple retrieved chunks into one contextual text block.

        The resulting context string is provided to the LLM to ground its answer generation.

        Args:
            chunks (List[Dict]): List of chunk dictionaries returned by the Searcher.

        Returns:
            str: A concatenated string of chunk texts separated by double newlines.
        """

        return "\n\n".join([c["text"] for c in chunks])
    

    
    def generate_answer(self, question:str, context:str)->str:
        """
        Generate a natural-language answer using a local Ollama model.

        This method constructs a structured prompt, sends it to the Ollama API,
        and returns the generated answer text.

        Args:
            question (str): The user's question or query.
            context (str): The concatenated context built from relevant chunks.

        Returns:
            str: The answer generated by the LLM.
        """
        prompt = f"Answer the question based only on the following context:\n {context}\n\nQuestion: {question}"
        response = ollama.chat(model=self.model, messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": prompt}
        ])
        return response['message']['content']
    

    def run(self, question:str):
        """
        Execute the complete answer generation pipeline for the current user.

        Steps:
            1. Retrieve relevant chunks using the Searcher (user-isolated).
            2. Re-rank the retrieved chunks using the Reranker.
            3. Build a single text context from the top chunks.
            4. Generate a final natural-language answer using the Ollama model.
            5. Log and print the results for visibility.

        Args:
            question (str): The user's question.

        Returns:
            None
        """

        self.logger.info(f" Generating answerfor user {self.user_id} for: {question}")
        results = self.searcher.search(question, top_k=20)
        reranker = Reranker()
        reranked_results = reranker.rerank(question, results, top_k=5)
        context = self.build_context(reranked_results)
        answer = self.generate_answer(question, context)
        print("\n Question:", question)
        print("\n Context:\n", context[:300], "...")
        print("\n Answer:\n", answer)
        self.logger.info(" Answer generated for user {self.user_id}.") 

